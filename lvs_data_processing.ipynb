{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTransformerRegistry.enable('default')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "import configparser\n",
    "import argparse \n",
    "from helpers import read\n",
    " \n",
    "import bottleneck as bn\n",
    "from LPA import Corpus, sockpuppet_distance\n",
    "from math import floor\n",
    "from scipy.spatial.distance import cdist, cityblock\n",
    "import matplotlib.pyplot as plt\n",
    "from visualize import sockpuppet_matrix, timeline\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Data successfully loaded from {file_path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None  # Important: Return None on error\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "    \n",
    "def unpivot_data(df, agg_column , var_name , value_name ,ignore_columns, processing_type,file_path2):  \n",
    "    if df is None:\n",
    "        print(\"Error: Input DataFrame is None. Skipping unpivot_data.\")\n",
    "        return None \n",
    "    try:\n",
    "        if processing_type == 'full':\n",
    "            pd.set_option('display.max_rows', None)\n",
    "            pd.set_option('display.max_columns', None) \n",
    "            # Drop the extra columns\n",
    "            df = df.drop(columns=ignore_columns, errors='ignore') # errors='ignore' prevents errors if columns don't exist \n",
    "            # Melt the DataFrame, using only 'Year' as the id_vars\n",
    "            df_melted = pd.melt(df, id_vars=[agg_column], var_name=var_name, value_name=value_name, value_vars=df.columns[1:])   \n",
    "            # Group by 'Year' and 'Cause' to sum the deaths across all entities\n",
    "            print(f\"Unpivoting data...\")\n",
    "            print(agg_column , var_name , value_name) \n",
    "            print(ignore_columns) \n",
    "            print(file_path2) \n",
    "        else:\n",
    "            df_melted = df\n",
    "            df_melted\n",
    "            # Melt the DataFrame, using only 'Year' as the id_vars\n",
    "\n",
    "\n",
    "        df_melted_grouped = df_melted.groupby([agg_column, var_name])[value_name].sum().reset_index()\n",
    "\n",
    "        # Calculate the total deaths per year\n",
    "        df_melted_grouped['Total_Per_Agg'] = df_melted_grouped.groupby(agg_column)[value_name].transform('sum')\n",
    "        # Calculate the relative deaths\n",
    "        # additional column to calculate the relative [Optional]  \n",
    "\n",
    "        if file_path2 == 'None':\n",
    "                            \n",
    "            df_melted_grouped['frequency_in_document'] = df_melted_grouped[value_name] / df_melted_grouped['Total_Per_Agg']\n",
    "        else:   \n",
    "            \n",
    "            df2 = pd.read_csv(file_path2)   \n",
    "            # Merge with the population data\n",
    "            df_melted_grouped  = pd.merge(df_melted_grouped, df2, left_on=agg_column, right_on='Year', how='inner') \n",
    "            df_melted_grouped['frequency_in_document'] = df_melted_grouped[value_name] / df_melted_grouped['Population']\n",
    "\n",
    "        # Rename\n",
    "        df_melted_grouped = df_melted_grouped.rename(columns={agg_column:'document',\n",
    "                                var_name: 'element'}) \n",
    "        return df_melted_grouped  \n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Column not found: {e}.  Check your 'agg_column', 'var_name', 'value_name', and 'ignore_columns' parameters.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during unpivoting: {e}\")\n",
    "        return None\n",
    "def clean_data(df,columns_to_keep):\n",
    "    \"\"\"Cleans the DataFrame (e.g., keep only relevant columns , handles missing values, data type conversions).\"\"\"\n",
    "    if df is None:\n",
    "        print(\"Error: Input DataFrame is None. Skipping clean_data.\")\n",
    "        return None, None\n",
    "    try:\n",
    "        print(\"Cleaning data...\")\n",
    "        df_cleaned = df.dropna()\n",
    "        # Keep only the relevant columns\n",
    "        df_cleaned = df_cleaned[columns_to_keep]\n",
    "        # Shorten the element names\n",
    "        unique_elements = df['element'].unique()\n",
    "        element_to_code = { element: f'E{i}' for i,  element  in enumerate(unique_elements) }\n",
    "        df_cleaned['element'] = df_cleaned['element'].map(element_to_code)  \n",
    "        # Create a DataFrame from the dictionary\n",
    "        entity_code_df = pd.DataFrame(list(element_to_code.items()), columns=['element_name', 'element']) \n",
    "        #df_cleaned['amount'] = pd.to_numeric(df_cleaned['amount'], errors='coerce')\n",
    "        return df_cleaned, entity_code_df\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Column not found: {e}. Check your 'columns_to_keep' parameter.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data cleaning: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "def filter_data(df, column, condition):\n",
    "    \"\"\"Filters the DataFrame based on a condition.\"\"\"\n",
    "    print(f\"Filtering data where {column} {condition}\")\n",
    "    return df[df[column] > condition]\n",
    "\n",
    "def calculate_summary(df, group_by_column, aggregation):\n",
    "    \"\"\"Calculates summary statistics on the DataFrame.\"\"\"\n",
    "    print(f\"Calculating summary by {group_by_column}...\")\n",
    "    return df.groupby(group_by_column).agg(aggregation)\n",
    "\n",
    "def save_results(df,entity_code_df, output_path,output_dic):\n",
    "    if df is None:\n",
    "        print(\"Error: Input DataFrame is None. Skipping save_results.\")\n",
    "        return\n",
    "\n",
    "    \"\"\"Saves the processed DataFrames to a CSV file.\"\"\"\n",
    "    print(f\"Saving results to: {output_path} and {output_dic}\") \n",
    "    try: \n",
    "        print(f\"Saving results to: {output_path} and {output_dic}\")\n",
    "        df.to_csv(output_path, index=False)  # Don't include the index\n",
    "        print(f\"Data successfully saved to {output_path}\")\n",
    "\n",
    "        if output_dic:\n",
    "            #  Create a DataFrame from the dictionary and save it.  Important for consistent structure.\n",
    "            entity_code_df.to_csv(output_dic, index=False)            \n",
    "            print(f\"Dictionary successfully saved to {output_path.replace('.csv', '_dict.csv')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def generate_signatures(df, entity_code_df, sig_file, dataset,graph,top):\n",
    "    \"\"\"\n",
    "    Generates and saves document signatures, along with related analyses and visualizations.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing document data.\n",
    "        entity_code_df (pd.DataFrame, optional): DataFrame mapping entity codes to names.\n",
    "        sig_file (str, optional): Path to save the signature DataFrame.\n",
    "        dataset (str): Name of the dataset for output directory.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        print(\"Error: Input DataFrame is None. Skipping generate_signatures.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Create results directory if it doesn't exist\n",
    "        import os\n",
    "        os.makedirs(f\"results/{dataset}\", exist_ok=True)\n",
    "\n",
    "        corpus = Corpus(df, \"document\", \"element\", \"frequency_in_document\")\n",
    "        dvr = corpus.create_dvr(equally_weighted=True)\n",
    "        top = int(top)\n",
    "        sigs = corpus.create_signatures(distance=\"JSD\",most_significant=top,prevalent=0.1)\n",
    "\n",
    "        #  Saving top N changed elements\n",
    "        sigs[1].to_csv(f\"results/{dataset}/top_{top}_most_changed.csv\")\n",
    "        sig = pd.DataFrame(sigs[1])\n",
    "\n",
    "        # Rename columns based on entity_code_df if provided\n",
    "        if entity_code_df is not None:\n",
    "            entity_code_to_name = entity_code_df.set_index(\"element\")[\"element_name\"].to_dict()\n",
    "            new_columns = [\n",
    "                entity_code_to_name.get(col, col) for col in sig.columns\n",
    "            ]  # Use get() for safety\n",
    "            sig.columns = new_columns\n",
    "            sig.to_csv(f\"results/{dataset}/top_{top}_most_changed_real_names.csv\")\n",
    "\n",
    "        # Save signatures if sig_file is provided\n",
    "        if sig_file:\n",
    "            ndf = pd.DataFrame(sigs[0])\n",
    "            ndf.to_csv(sig_file, index=True)\n",
    "            print(f\"Signatures successfully saved to {sig_file}\")\n",
    "\n",
    "        # Save element list\n",
    "        with open(f\"results/{dataset}/list.txt\", \"w\") as f:\n",
    "            for item in sigs[0]:\n",
    "                f.write(f\"{item}\\n\")\n",
    "\n",
    "        # Sockpuppet analysis\n",
    "        if graph == 'True':\n",
    "            ecorpus = Corpus(df)\n",
    "            ecorpus_dvr = ecorpus.create_dvr(equally_weighted=True)  # Corrected variable name\n",
    "            esigs = ecorpus.create_signatures(distance=\"JSD\")\n",
    "            espd = sockpuppet_distance(ecorpus, ecorpus, heuristic=False, distance=\"euclidean\")\n",
    "            chart = sockpuppet_matrix(espd)\n",
    "            if chart is not None:\n",
    "                try:\n",
    "                    chart.save(f\"results/{dataset}/sockpuppet_distance_matrix.png\", scale_factor=4.0)\n",
    "                    print(f\"Sockpuppet distance matrix chart saved to results/{dataset}/sockpuppet_distance_matrix.png\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving sockpuppet distance matrix chart: {e}\")\n",
    "\n",
    "            espd.to_csv(f\"results/{dataset}/sockpuppet_distance_matrix.csv\", index=False)\n",
    "\n",
    "            # Top 10 distances chart\n",
    "            try:\n",
    "                top_changing = sig[sig.sum(0).abs().sort_values(ascending=False).head(10).index]\n",
    "                chart = (\n",
    "                    alt.Chart(\n",
    "                        top_changing.reset_index()\n",
    "                        .melt(id_vars=\"index\")\n",
    "                        .rename(\n",
    "                            columns={\n",
    "                                \"index\": \"Year\",\n",
    "                                \"variable\": \"Element\",\n",
    "                                \"value\": \"Distance from PM\",\n",
    "                            }\n",
    "                        )\n",
    "                    )\n",
    "                    .mark_line()\n",
    "                    .encode(x=\"Year:N\", y=\"Distance from PM\", color=\"Element\")\n",
    "                    .properties(width=300, height=300, title=\"\")\n",
    "                )\n",
    "                chart.save(f\"results/{dataset}/top_10_distances.png\", scale_factor=4.0)\n",
    "                print(f\"Top 10 distances chart saved to results/{dataset}/top_10_distances.png\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating or saving top 10 distances chart: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failure in generate_signatures: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline  \n",
    "# Reuse the functions from the basic example\n",
    "# clean_data, filter_data, calculate_summary, save_results\n",
    "\n",
    "def process_data(file_path, file_path2, ignore_columns, columns_to_keep, agg_column, var_name, value_name, output_path, output_dic, processing_type, sig_file,dataset,graph,top):\n",
    "    \"\"\"\n",
    "    Pipeline function to load, unpivot, clean, and save data.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "        file_path2 (str): Path to the second input CSV file.\n",
    "        ignore_columns (list): List of columns to ignore during unpivoting.\n",
    "        columns_to_keep (dict): Columns to keep and their new names.\n",
    "        agg_column (str): Column to aggregate by during unpivoting.\n",
    "        var_name (str): Name for the variable column after unpivoting.\n",
    "        value_name (str): Name for the value column after unpivoting.\n",
    "        output_path (str): Path to save the processed CSV file.\n",
    "        output_dic (dict, optional): Dictionary to save as a CSV file.\n",
    "    \"\"\"\n",
    "    df = load_data(file_path)\n",
    "    if df is None:\n",
    "        print(\"Pipeline aborted due to error in load_data.\")\n",
    "        return  # Stop the pipeline\n",
    "\n",
    "    df_unpivoted = unpivot_data(df, agg_column, var_name, value_name, ignore_columns,processing_type, file_path2)\n",
    "    if df_unpivoted is None:\n",
    "        print(\"Pipeline aborted due to error in unpivot_data.\")\n",
    "        return\n",
    "\n",
    "    df_cleaned ,entity_code_df = clean_data(df_unpivoted, columns_to_keep)\n",
    "    # print(df_cleaned  ) \n",
    "    if df_cleaned is None:\n",
    "        print(\"Pipeline aborted due to error in clean_data.\")  \n",
    "        return\n",
    " \n",
    "    save_results(df_cleaned,entity_code_df, output_path, output_dic)\n",
    "    print(\"Pipeline execution complete!\")\n",
    "\n",
    "    generate_signatures(df_cleaned,entity_code_df,sig_file,dataset,graph,top)  \n",
    "    print(\"signatures execution complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from data/sotu/0.csv\n",
      "Cleaning data...\n",
      "Saving results to: data/sotu/lvs.csv and data/sotu/dic.csv\n",
      "Saving results to: data/sotu/lvs.csv and data/sotu/dic.csv\n",
      "Data successfully saved to data/sotu/lvs.csv\n",
      "Dictionary successfully saved to data/sotu/lvs_dict.csv\n",
      "Pipeline execution complete!\n",
      "Signatures successfully saved to data/sotu/signatures.csv\n",
      "signatures execution complete!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 1. Set up argument parser\n",
    "    #parser = argparse.ArgumentParser(description=\"Process data from a CSV file.\")\n",
    "    #parser.add_argument(\"--config\", help=\"Path to the config file\", default=\"config.toml\")\n",
    "    #args = parser.parse_args()\n",
    "    config_file_path = 'config_sotu.toml'  # Replace with your actual path\n",
    "\n",
    "    # 2. Read the config file\n",
    "    config = configparser.ConfigParser()\n",
    "    #config.read(args.config)\n",
    "    config.read(config_file_path)\n",
    "    # 3. Get parameters from the config\n",
    "    file_path = config.get(\"data\", \"file_path\")\n",
    "    file_path2 = config.get(\"data\", \"file_path2\")    \n",
    "    agg_column=config.get(\"proc\",\"agg_column\")\n",
    "    var_name=config.get(\"proc\",\"var_name\") \n",
    "    value_name=config.get(\"proc\",\"value_name\")  \n",
    "    processing_type = config.get(\"proc\",\"processing_type\")\n",
    "    output_path = config.get(\"output\", \"output_path\") \n",
    "    output_dic = config.get(\"output\", \"output_dic\")  \n",
    "    sig_file = config.get(\"output\", \"sig_file\") \n",
    "    dataset = config.get(\"data\", \"dataset\")\n",
    "    graph = config.get(\"output\", \"graph\")\n",
    "    top = config.get(\"output\", \"top\")\n",
    "    # constants \n",
    "    ignore_columns = ['Entity','Code']\n",
    "    columns_to_keep = ['document', 'element', 'frequency_in_document'] \n",
    "    # 4. Call the processing function\n",
    "    process_data(file_path,file_path2,ignore_columns,columns_to_keep,agg_column,var_name,value_name,output_path,output_dic,processing_type,sig_file,dataset,graph,top) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
